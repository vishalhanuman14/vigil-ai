# QVigil - AI Monitoring Application

QVigil is a privacy-first monitoring application that uses on-device AI models for video analysis and intelligent alert generation. The application combines Qualcomm's VideoMAE model for activity recognition with Large Language Models for context-aware decision making.

## ğŸš€ Features

- **Privacy-First**: All video processing happens locally on your device
- **Real-time Monitoring**: Live webcam feed with continuous analysis
- **AI-Powered Alerts**: Context-aware notifications based on user-defined goals
- **Activity Recognition**: Uses Qualcomm's VideoMAE model for understanding activities
- **Customizable Zones**: Define specific areas for monitoring
- **Cross-Platform**: Built with Electron for Windows, macOS, and Linux support

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ Frontend (React + Electron)
â”‚   â”œâ”€â”€ Landing page with onboarding
â”‚   â”œâ”€â”€ Real-time video display
â”‚   â”œâ”€â”€ Alert dashboard
â”‚   â””â”€â”€ Configuration interface
â”‚
â”œâ”€â”€ Backend (Node.js + Python)
â”‚   â”œâ”€â”€ Video capture and processing
â”‚   â”œâ”€â”€ Python service for VideoMAE inference
â”‚   â”œâ”€â”€ LLM integration for alert decisions
â”‚   â””â”€â”€ Inter-process communication
â”‚
â””â”€â”€ AI Models
    â”œâ”€â”€ Qualcomm VideoMAE (video-mae)
    â””â”€â”€ Llama 3.2 3B Instruct
```

## ğŸ“‹ Prerequisites

- **Python 3.11**
- **Node.js 18+**
- **npm** or **yarn**
- **Git**

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd vigil-llm
```

### 2. Set up Python Environment

```bash
# Create and activate virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install Python dependencies
pip install -r requirements.txt
pip install "qai-hub-models[video-mae]"
```

### 3. Install Llama 3.2 Model

Follow the instructions at: https://huggingface.co/qualcomm/Llama-v3.2-3B-Instruct

**Note**: Model assets require proper licensing and may not be readily available for download due to licensing restrictions.

### 4. Install Node.js Dependencies

```bash
npm install
```

## ğŸš€ Development

### Start Development Server

```bash
# Ensure Python virtual environment is activated
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Start the application in development mode
npm run dev
```

This will:
- Start the Vite development server for the React frontend
- Launch the Electron application
- Initialize the Python VideoMAE service

## ğŸƒâ€â™‚ï¸ Usage

1. **Launch the Application**: Run `npm run dev` to start the development environment

2. **Setup Monitoring Goal**: 
   - Describe what you want to monitor (e.g., "Monitor my sleeping baby for signs of distress")
   - The AI will generate a context for relevant events to watch

3. **Configure Safe Zones** (Optional):
   - Click the pencil icon to define specific areas for monitoring
   - Draw a polygon over areas of interest

4. **Start Monitoring**:
   - Click "Continue" to begin real-time analysis
   - The system will capture video clips every 5 seconds
   - Activities are analyzed using VideoMAE
   - LLM determines if alerts should be triggered

5. **Monitor Events**:
   - Live status shows current detected activities
   - Event log displays timestamped alerts
   - All processing happens locally for privacy

## ğŸ“ Project Structure

```
vigil-llm/
â”œâ”€â”€ assets/                 # Static assets and images
â”œâ”€â”€ electron/              # Electron main process
â”‚   â”œâ”€â”€ backend/           # Node.js backend logic
â”‚   â”‚   â”œâ”€â”€ ai_handler.js  # LLM integration
â”‚   â”‚   â””â”€â”€ python_service.js # Python model interface
â”‚   â”œâ”€â”€ main.js           # Electron main process
â”‚   â””â”€â”€ preload.js        # Electron preload script
â”œâ”€â”€ scripts/              # Python scripts
â”‚   â”œâ”€â”€ video_model_service.py # VideoMAE service
â”‚   â””â”€â”€ run_video_model.py     # VideoMAE CLI runner
â”œâ”€â”€ src/                  # React frontend
â”‚   â”œâ”€â”€ Context/          # React contexts
â”‚   â”œâ”€â”€ Home/            # Main monitoring interface
â”‚   â”œâ”€â”€ Landing/         # Landing page
â”‚   â”œâ”€â”€ Configure/       # Zone configuration
â”‚   â””â”€â”€ App.jsx         # Main React component
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ package.json        # Node.js dependencies
â””â”€â”€ README.md          # This file
```

## ğŸ”§ Configuration

### Video Settings
- **Recording Interval**: 5 seconds (configurable in `src/Home/index.jsx`)
- **Clip Duration**: 4 seconds (configurable in `src/Home/index.jsx`)
- **Video Format**: WebM (browser capture)

### AI Models
- **VideoMAE**: Qualcomm's optimized model for activity recognition
- **LLM**: Llama 3.2 3B Instruct for decision making
- **Fallback**: Gemini API for cloud-based processing

## ğŸ”’ Privacy & Security

- **Local Processing**: All video analysis happens on-device
- **No Cloud Storage**: Video clips are processed locally and discarded
- **Minimal Network**: Only LLM API calls for alert decisions (optional)
- **User Control**: Full control over monitoring goals and zones

## ğŸ› Troubleshooting

### Common Issues

1. **Python Service Not Starting**:
   ```bash
   # Check if Python virtual environment is activated
   which python
   # Should point to your venv/bin/python
   ```

2. **VideoMAE Model Loading Issues**:
   ```bash
   # Test the model separately
   python scripts/run_video_model.py --video path/to/test/video.mp4
   ```

3. **Webcam Permission Issues**:
   - Ensure browser/Electron has camera permissions
   - Check system privacy settings

4. **Missing Dependencies**:
   ```bash
   # Reinstall Python dependencies
   pip install -r requirements.txt --force-reinstall
   
   # Reinstall Node dependencies
   npm install
   ```

### Debug Modes

- **Development**: `npm run dev` includes Chrome DevTools
- **Python Logs**: Check terminal output for VideoMAE service logs
- **Electron Logs**: Available in DevTools console

## ğŸ“¦ Building for Production

```bash
# Build the React frontend
npm run build

# Package the Electron application (requires additional electron-builder setup)
# npm run electron:pack
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIR License

## ğŸ”— References

- [Qualcomm AI Hub Models](https://github.com/quic/ai-hub-models)
- [VideoMAE Model](https://huggingface.co/qualcomm/Llama-v3.2-3B-Instruct)
- [Electron Documentation](https://www.electronjs.org/docs)
- [React Documentation](https://reactjs.org/docs)

## âš ï¸ Important Notes

- The Llama 3.2 model requires proper licensing and may not be readily available
- Some features require API keys for cloud-based LLM services
- This is a development/demo application - production deployment requires additional security considerations
